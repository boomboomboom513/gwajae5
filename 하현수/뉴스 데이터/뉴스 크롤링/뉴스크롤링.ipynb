{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'findAll'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 64\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m             last \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m Crawling(\u001b[39m'\u001b[39;49m\u001b[39m하이닉스\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m2023.06.19\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[3], line 35\u001b[0m, in \u001b[0;36mCrawling\u001b[1;34m(keyword, date)\u001b[0m\n\u001b[0;32m     32\u001b[0m soup \u001b[39m=\u001b[39m bs(html, \u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     34\u001b[0m ul \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39mfind(\u001b[39m'\u001b[39m\u001b[39mul\u001b[39m\u001b[39m'\u001b[39m, {\u001b[39m'\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39mtype01\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[1;32m---> 35\u001b[0m li_list \u001b[39m=\u001b[39m ul\u001b[39m.\u001b[39;49mfindAll(\u001b[39m'\u001b[39m\u001b[39mli\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[39mfor\u001b[39;00m li \u001b[39min\u001b[39;00m li_list:\n\u001b[0;32m     38\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'findAll'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import feedparser\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import urllib\n",
    "import urllib.request as req\n",
    "import requests\n",
    "from konlpy.tag import Kkma, Okt, Komoran\n",
    "okt = Okt()\n",
    "from konlpy.utils import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "href_list = []   # 기사 주소가 들어갈 리스트\n",
    "TitDesc_list = []   # 제목 + 요약 내용 리스트\n",
    "title_list = []   # 제목 리스트\n",
    "\n",
    "\n",
    "def Crawling(keyword, date):\n",
    "    keyword = '+'.join(keyword.split(' '))\n",
    "\n",
    "    last = False\n",
    "    page_num = 1\n",
    "\n",
    "    ds = date\n",
    "    de = ds\n",
    "    while last == False:\n",
    "        url = \"https://search.naver.com/search.naver?&where=news&query={0}&sm=tab_pge&sort=0&photo=0&field=0&reporter_article=&pd=3&ds={1}&de={2}&docid=&nso=so:r,p:,a:all&mynews=1&cluster_rank=238&start={3}&refresh_start=0\".format(keyword,ds,de,str(page_num))\n",
    "        raw = requests.get(url)\n",
    "        html = raw.content\n",
    "        soup = bs(html, 'html.parser')\n",
    "\n",
    "        ul = soup.find('ul', {'class':'type01'})\n",
    "        li_list = ul.findAll('li')\n",
    "\n",
    "        for li in li_list:\n",
    "            try:\n",
    "                href_list.append(li.dl.dt.a['href'])\n",
    "\n",
    "                d_list = li.findAll('')\n",
    "                #자식 노드 dd가 두 개인데 두 번째 노드에 description이 들어가있으므로 전부 불러온 후 인덱싱할 것임\n",
    "\n",
    "                title = li.dl.dt.a['title']\n",
    "                description = d_list[1].text\n",
    "                # 자식 노드 dd의 두 번째에 들어가 있는 description을 text로 불러옴\n",
    "\n",
    "                title_list.append(title)\n",
    "                TitDesc_list.append( title + ' ' + description)\n",
    "                # 제목과 요약내용을 붙여서 리스트에 넣음 \n",
    "            except AttributeError:\n",
    "                pass\n",
    "\n",
    "        \n",
    "        # 마지막 페이지 주소 확인 (다음페이지 버튼이 없으면 종료페이지로 간주)\n",
    "        page = soup.find('div', {'class':'paging'})\n",
    "        page_a_list = page.findAll('a')\n",
    "        if '다음페이지' in str(page_a_list[-1]):\n",
    "            page_num += 10\n",
    "        else:\n",
    "            last = True\n",
    "            \n",
    "\n",
    "Crawling('하이닉스', '2023.06.19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pandas-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
