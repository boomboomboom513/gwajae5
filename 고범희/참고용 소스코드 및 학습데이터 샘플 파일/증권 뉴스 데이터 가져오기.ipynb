{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from urllib.request import urlopen\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 종목 뉴스리스트 페이지별로 전체 html코드를 긁어온 후, 리스트 형식으로 반환\n",
    "def getData(codeList, newsPage):\n",
    "    pageList = []\n",
    "    for code in codeList:\n",
    "        for page in range(newsPage, newsPage+1):\n",
    "            newsListPageUrl = 'https://finance.naver.com/item/news_news.naver?code='+str(code)+'&page='+str(page)+'&sm=title_entity_id.basic&clusterId='\n",
    "            data = requests.get(newsListPageUrl)\n",
    "            soup = BeautifulSoup(data.content.decode('euc-kr', 'replace'), 'html.parser')\n",
    "            dataFilter = soup.find('tbody')\n",
    "            pageList.append(dataFilter)\n",
    "    return pageList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스페이지별로 전체 html코드를 가공하여 각 뉴스기사들의 url주소를 만들어서, 리스트 형식으로 반환\n",
    "def change(bs4Data, num):\n",
    "    makeUrlList = []\n",
    "    newsUrlList = []\n",
    "    for bs4 in bs4Data:\n",
    "        change = bs4.select('tbody > tr > td > a')\n",
    "        for i in range(len(change)):\n",
    "            if num <= 9:\n",
    "                newsUrlList.append('https://finance.naver.com'+str(change[i])[21:65]+str(change[i])[69:83]+str(change[i])[87:99]+str(change[i])[103:110]+str(change[i])[114:138])\n",
    "            elif num <= 99:\n",
    "                newsUrlList.append('https://finance.naver.com'+str(change[i])[21:65]+str(change[i])[69:83]+str(change[i])[87:99]+str(change[i])[103:111]+str(change[i])[115:139])\n",
    "            elif num <= 999:\n",
    "                newsUrlList.append('https://finance.naver.com'+str(change[i])[21:65]+str(change[i])[69:83]+str(change[i])[87:99]+str(change[i])[103:112]+str(change[i])[116:140])\n",
    "    return newsUrlList"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:고범희/증권 뉴스 데이터 가져오기.ipynb
   "execution_count": 9,
=======
   "execution_count": 4,
>>>>>>> 51eaaeca61db9de8f77ff83231d4cc6500c23eea:고범희/참고용 소스코드 및 학습데이터 샘플 파일/증권 뉴스 데이터 가져오기.ipynb
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNewsData(urlLists):\n",
    "    newsDateList = []\n",
    "    newsTitleList = []\n",
    "    newsTextList = []\n",
    "    for i in range(len(urlLists)):\n",
    "        html = requests.get(urlLists[i])\n",
    "        soup = BeautifulSoup(html.content.decode('euc-kr', 'replace'), 'html.parser')\n",
    "        newsDateList.append(soup.find('span',{'class':'tah p11'}).get_text())\n",
    "        newsTitleList.append(soup.find('strong', {'class':'c p15'}).get_text())\n",
    "        newsText = soup.find('div', {'class':'scr01'})\n",
    "        if newsText.find('div', {'class':'link_news'}) != None:\n",
    "            newsText.find('div', {'class':'link_news'}).decompose()\n",
    "        newsTextList.append(newsText.get_text()[1:-1])\n",
    "    newsDf = pd.DataFrame({\n",
    "        '날짜':newsDateList\n",
    "        , '제목':newsTitleList\n",
    "        , '내용':newsTextList\n",
    "    })\n",
    "    return newsDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filescan():\n",
    "    fileDataList = []\n",
    "    fileList = glob('./newsdata/*.csv')\n",
    "    for i in range(len(fileList)):\n",
    "        fileDataList.append(pd.read_csv('./newsdata/'+fileList[i][11:], encoding='utf-8'))\n",
    "    hapData = pd.concat(fileDataList)\n",
    "    hapData.drop_duplicates(['제목'], inplace=True)\n",
    "    hapData.sort_values(['날짜'], inplace=True)\n",
    "    hapData.reset_index(inplace=True)\n",
    "    hapData.drop('index', axis=1, inplace=True)\n",
    "    hapData.to_csv('./newsdata/루닛 뉴스 데이터.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openFile():\n",
    "    filtering = pd.read_csv('./newsdata/루닛 뉴스 데이터.csv', encoding='utf-8')\n",
    "    for i in range(0, len(filtering)):\n",
    "        filtering['날짜'][i] = filtering['날짜'][i].strip()\n",
    "        filtering['내용'][i] = str(filtering['내용'][i]).replace('\\t','')\n",
    "    filtering.to_csv('./newsdata/루닛 뉴스 데이터.csv', encoding='utf-8', index=False)\n",
    "    return pd.read_csv('./newsdata/루닛 뉴스 데이터.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:고범희/증권 뉴스 데이터 가져오기.ipynb
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: '..\\newsdata'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m     urlList \u001b[39m=\u001b[39m change(bs4Data, numberCount)\n\u001b[0;32m      7\u001b[0m     getNewsDf \u001b[39m=\u001b[39m getNewsData(urlList)\n\u001b[1;32m----> 8\u001b[0m     getNewsDf\u001b[39m.\u001b[39;49mto_csv(\u001b[39m'\u001b[39;49m\u001b[39m../newsdata/하이닉스 뉴스 데이터\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m+\u001b[39;49m\u001b[39mstr\u001b[39;49m(numberCount)\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, encoding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m'\u001b[39;49m, index\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m) \u001b[39m# 원하는 종목 코드에 맞는 종목 이름으로 꼭 바꿔서 사용하세요.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39m# 진행도는 여기서 확인할 수 없으니, ./newsdata/안에 있는 파일 생성되는 것을 확인하세요. -> 네이버 기사 페이지 숫자가 파일명에 붙어서 만들어집니다.\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39m# 예) 하이닉스 뉴스 데이터77.csv -> 하이닉스 뉴스 77페이지 뉴스 리스트 다 긁어옴\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39m# 네이버 봇이 자동으로 막는 문제인지 알 수 없으나, 30~50개 정도 파일을 만들어낼때마다 'Attribute error'가 뜨는데\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39m# 에러가 안뜰 수도 있음.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m# 테스트 결과 하이닉스 총 344페이지 뉴스 페이지 전부 가져와지는 것을 확인함.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m filescan()\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\py32\\lib\\site-packages\\pandas\\core\\generic.py:3772\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3761\u001b[0m df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, ABCDataFrame) \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_frame()\n\u001b[0;32m   3763\u001b[0m formatter \u001b[39m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3764\u001b[0m     frame\u001b[39m=\u001b[39mdf,\n\u001b[0;32m   3765\u001b[0m     header\u001b[39m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3769\u001b[0m     decimal\u001b[39m=\u001b[39mdecimal,\n\u001b[0;32m   3770\u001b[0m )\n\u001b[1;32m-> 3772\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[39m.\u001b[39;49mto_csv(\n\u001b[0;32m   3773\u001b[0m     path_or_buf,\n\u001b[0;32m   3774\u001b[0m     lineterminator\u001b[39m=\u001b[39;49mlineterminator,\n\u001b[0;32m   3775\u001b[0m     sep\u001b[39m=\u001b[39;49msep,\n\u001b[0;32m   3776\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m   3777\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m   3778\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m   3779\u001b[0m     quoting\u001b[39m=\u001b[39;49mquoting,\n\u001b[0;32m   3780\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[0;32m   3781\u001b[0m     index_label\u001b[39m=\u001b[39;49mindex_label,\n\u001b[0;32m   3782\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[0;32m   3783\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[0;32m   3784\u001b[0m     quotechar\u001b[39m=\u001b[39;49mquotechar,\n\u001b[0;32m   3785\u001b[0m     date_format\u001b[39m=\u001b[39;49mdate_format,\n\u001b[0;32m   3786\u001b[0m     doublequote\u001b[39m=\u001b[39;49mdoublequote,\n\u001b[0;32m   3787\u001b[0m     escapechar\u001b[39m=\u001b[39;49mescapechar,\n\u001b[0;32m   3788\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m   3789\u001b[0m )\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\py32\\lib\\site-packages\\pandas\\io\\formats\\format.py:1186\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1165\u001b[0m     created_buffer \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1167\u001b[0m csv_formatter \u001b[39m=\u001b[39m CSVFormatter(\n\u001b[0;32m   1168\u001b[0m     path_or_buf\u001b[39m=\u001b[39mpath_or_buf,\n\u001b[0;32m   1169\u001b[0m     lineterminator\u001b[39m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1184\u001b[0m     formatter\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfmt,\n\u001b[0;32m   1185\u001b[0m )\n\u001b[1;32m-> 1186\u001b[0m csv_formatter\u001b[39m.\u001b[39;49msave()\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1189\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\py32\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:240\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[39mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[39m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[0;32m    241\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilepath_or_buffer,\n\u001b[0;32m    242\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    243\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    244\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors,\n\u001b[0;32m    245\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompression,\n\u001b[0;32m    246\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstorage_options,\n\u001b[0;32m    247\u001b[0m ) \u001b[39mas\u001b[39;00m handles:\n\u001b[0;32m    248\u001b[0m     \u001b[39m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter \u001b[39m=\u001b[39m csvlib\u001b[39m.\u001b[39mwriter(\n\u001b[0;32m    250\u001b[0m         handles\u001b[39m.\u001b[39mhandle,\n\u001b[0;32m    251\u001b[0m         lineterminator\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    256\u001b[0m         quotechar\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquotechar,\n\u001b[0;32m    257\u001b[0m     )\n\u001b[0;32m    259\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save()\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\py32\\lib\\site-packages\\pandas\\io\\common.py:737\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[39m# Only for write methods\u001b[39;00m\n\u001b[0;32m    736\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode \u001b[39mand\u001b[39;00m is_path:\n\u001b[1;32m--> 737\u001b[0m     check_parent_directory(\u001b[39mstr\u001b[39;49m(handle))\n\u001b[0;32m    739\u001b[0m \u001b[39mif\u001b[39;00m compression:\n\u001b[0;32m    740\u001b[0m     \u001b[39mif\u001b[39;00m compression \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mzstd\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    741\u001b[0m         \u001b[39m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\py32\\lib\\site-packages\\pandas\\io\\common.py:600\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    598\u001b[0m parent \u001b[39m=\u001b[39m Path(path)\u001b[39m.\u001b[39mparent\n\u001b[0;32m    599\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m parent\u001b[39m.\u001b[39mis_dir():\n\u001b[1;32m--> 600\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39mrf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot save file into a non-existent directory: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mparent\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: Cannot save file into a non-existent directory: '..\\newsdata'"
     ]
    }
   ],
   "source": [
    "stockCodeList = ['000660'] # 원하는 종목\n",
    "stockNewsPage = 15       # 원하는 뉴스페이지 숫자 1 ~ 999페이지까지만 긁어오기 가능\n",
=======
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>날짜</th>\n",
       "      <th>제목</th>\n",
       "      <th>내용</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022.07.19 09:34</td>\n",
       "      <td>AI 진단기업 루닛 21일 상장, 투자자들 유의할 점은</td>\n",
       "      <td>피어그룹 AI 진단 상장사 제외상장 직후 유통 가능 물량 49%2025년 순이익 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022.07.19 19:06</td>\n",
       "      <td>거래소 \"루닛 신규 상장 승인\"…기술특례로 코스닥 입성</td>\n",
       "      <td>서울 여의도 한국거래소 전경. 2015.7.2/뉴스1 © News1 박세연 기자(서...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022.07.20 16:14</td>\n",
       "      <td>루닛·에이프릴, 흥행실패에 조달규모도 축소…어떤 투자 줄였나</td>\n",
       "      <td>[이달 코스닥 상장 앞둬공모가 희망 최하단보다 20~30% ↓]루닛, 에이프릴바이...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022.07.21 09:09</td>\n",
       "      <td>[특징주]루닛, 시초가 공모가 소폭 웃돌아…10%대 상승중</td>\n",
       "      <td>[이데일리 안혜신 기자] 루닛(328130)이 공모가를 소폭 웃도는 수준에서 시초...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022.07.21 09:13</td>\n",
       "      <td>[특징주] 루닛, 코스닥 상장 첫날 ‘상승’</td>\n",
       "      <td>루닛 주가가 코스닥 입성 첫날 급등하고 있다.21일 오전 9시 11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>2023.07.05 14:12</td>\n",
       "      <td>3만원→18만원된 루닛…초기 투자자 팔았는데도 주가 ‘고공행진’</td>\n",
       "      <td>루닛 2대 주주 웰어라이크 60만주 장내매도향후 초기 투자자 등 차익 실현 가능성 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>2023.07.05 14:55</td>\n",
       "      <td>루닛, 사우디 가상병원 프로젝트 참여…'AI 암 검진’ 솔루션 제공</td>\n",
       "      <td>빈 살만 사우디 왕자 ‘SEHA 가상병원’ 프로젝트  루닛의 흉부·유방촬영술 AI ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>2023.07.05 16:34</td>\n",
       "      <td>[주가동향] \"187에 탑승했는데 잘한건가요\" 루닛 주가 고점 앞두고 개미 의견 분분</td>\n",
       "      <td>루닛 주가 20만원 임박고점 앞두고 개미 의견 분분언론보도 힘입은 급등 패턴개인투자...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>2023.07.09 18:48</td>\n",
       "      <td>큰손들, 금양·루닛·현대로템도 담았네</td>\n",
       "      <td>마켓 PRO 부자들의 추천종목대형 증권사 고액 자산가 고객들이 삼성전자와 포스코홀딩...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>2023.07.19 11:48</td>\n",
       "      <td>루닛, 글로벌 학회서 잇단 압도적 기술력 입증...주가선행지표</td>\n",
       "      <td>글로벌 톱티어 AI 학회 매년 참석하는 의료AI 업체는 루닛·패스AI뿐AI 학회 참...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>406 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   날짜                                               제목  \\\n",
       "0    2022.07.19 09:34                   AI 진단기업 루닛 21일 상장, 투자자들 유의할 점은   \n",
       "1    2022.07.19 19:06                   거래소 \"루닛 신규 상장 승인\"…기술특례로 코스닥 입성   \n",
       "2    2022.07.20 16:14                루닛·에이프릴, 흥행실패에 조달규모도 축소…어떤 투자 줄였나   \n",
       "3    2022.07.21 09:09                 [특징주]루닛, 시초가 공모가 소폭 웃돌아…10%대 상승중   \n",
       "4    2022.07.21 09:13                         [특징주] 루닛, 코스닥 상장 첫날 ‘상승’   \n",
       "..                ...                                              ...   \n",
       "401  2023.07.05 14:12              3만원→18만원된 루닛…초기 투자자 팔았는데도 주가 ‘고공행진’   \n",
       "402  2023.07.05 14:55            루닛, 사우디 가상병원 프로젝트 참여…'AI 암 검진’ 솔루션 제공   \n",
       "403  2023.07.05 16:34  [주가동향] \"187에 탑승했는데 잘한건가요\" 루닛 주가 고점 앞두고 개미 의견 분분   \n",
       "404  2023.07.09 18:48                             큰손들, 금양·루닛·현대로템도 담았네   \n",
       "405  2023.07.19 11:48               루닛, 글로벌 학회서 잇단 압도적 기술력 입증...주가선행지표   \n",
       "\n",
       "                                                    내용  \n",
       "0    피어그룹 AI 진단 상장사 제외상장 직후 유통 가능 물량 49%2025년 순이익 5...  \n",
       "1    서울 여의도 한국거래소 전경. 2015.7.2/뉴스1 © News1 박세연 기자(서...  \n",
       "2     [이달 코스닥 상장 앞둬공모가 희망 최하단보다 20~30% ↓]루닛, 에이프릴바이...  \n",
       "3     [이데일리 안혜신 기자] 루닛(328130)이 공모가를 소폭 웃도는 수준에서 시초...  \n",
       "4             루닛 주가가 코스닥 입성 첫날 급등하고 있다.21일 오전 9시 11...  \n",
       "..                                                 ...  \n",
       "401  루닛 2대 주주 웰어라이크 60만주 장내매도향후 초기 투자자 등 차익 실현 가능성 ...  \n",
       "402  빈 살만 사우디 왕자 ‘SEHA 가상병원’ 프로젝트  루닛의 흉부·유방촬영술 AI ...  \n",
       "403  루닛 주가 20만원 임박고점 앞두고 개미 의견 분분언론보도 힘입은 급등 패턴개인투자...  \n",
       "404  마켓 PRO 부자들의 추천종목대형 증권사 고액 자산가 고객들이 삼성전자와 포스코홀딩...  \n",
       "405  글로벌 톱티어 AI 학회 매년 참석하는 의료AI 업체는 루닛·패스AI뿐AI 학회 참...  \n",
       "\n",
       "[406 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stockCodeList = ['328130'] # 원하는 종목\n",
    "stockNewsPage = 45         # 원하는 뉴스페이지 숫자 1 ~ 999페이지까지만 긁어오기 가능. # 원하는 페이지수까지에 1을 더해서 입력해야한다.\n",
>>>>>>> 51eaaeca61db9de8f77ff83231d4cc6500c23eea:고범희/참고용 소스코드 및 학습데이터 샘플 파일/증권 뉴스 데이터 가져오기.ipynb
    "\n",
    "for numberCount in range(33, stockNewsPage):\n",
    "    bs4Data = getData(stockCodeList, numberCount)\n",
    "    urlList = change(bs4Data, numberCount)\n",
    "    getNewsDf = getNewsData(urlList)\n",
    "    getNewsDf.to_csv('./newsdata/루닛 뉴스 데이터'+str(numberCount)+'.csv', encoding='utf-8', index=False) # 원하는 종목 코드에 맞는 종목 이름으로 꼭 바꿔서 사용하세요.\n",
    "\n",
    "# 진행도는 여기서 확인할 수 없으니, ./newsdata/안에 있는 파일 생성되는 것을 확인하세요. -> 네이버 기사 페이지 숫자가 파일명에 붙어서 만들어집니다.\n",
    "# 예) 하이닉스 뉴스 데이터77.csv -> 하이닉스 뉴스 77페이지 뉴스 리스트 다 긁어옴\n",
    "# 네이버 봇이 자동으로 막는 문제인지 알 수 없으나, 30~50개 정도 파일을 만들어낼때마다 'Attribute error'가 뜨는데\n",
    "# 하이닉스 뉴스 데이터93.csv 까지 파일이 만들어졌다가 에러가 떳다면\n",
    "# for numberCount in range(93, stockNewsPage) <- 이 코드에서 네이버 뉴스 페이지 수 93을 \n",
    "# 입력한채로 다시 실행하면 다시 만들기가 시작됩니다.\n",
    "# 에러가 안뜰 수도 있음.\n",
    "# 테스트 결과 하이닉스 총 344페이지 뉴스 페이지 전부 가져와지는 것을 확인함.\n",
    "\n",
    "filescan()\n",
    "openFile()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pandas-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
